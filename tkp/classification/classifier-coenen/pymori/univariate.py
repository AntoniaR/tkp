# By Thijs Coenen oktober 2007 for Research with the Transients Key Project
"""This module contains functions that are used to find univariate
splits in a set of data. It is not meant to be run stand alone."""
from core import initial_weights, multi_class_split
import random

def univariate_split(data, total_weight, MIN_LEAF_SIZE = 1):
    """This function looks for univariate splits in the learning sample. That
    is it looks for splits that take into account only the values of one of the
    features in the datapoints (after having looked for the best feature to 
    split on).
    
    input:
    data -- a list of tuples or lists with n + 3 entries for data that has 
    n variables. The other three entries, the last and the second to last 
    and the third to last are respectively reserved for the class label and the 
    weight and an unique id (needed during sorting, of which this library does a
    lot). The class labels are integers starting at 0 and going to n_class - 1. 
    total_weight -- list of floats (or integers), the total weight of all the 
    datapoints in the data (one entry for each class, indexed by class label).
    Passing this to the function avoids having a loop to recalculate it.
    MIN_LEAF_SIZE -- integer, minimum leaf node size for the decision tree 
    that is constructed by this function. Trees with too small leaf node sizes 
    generalize badly to new data.
        
    output: 
    result_dict -- a dictionary containing all the relevant data about the 
    split generated by this function. The keys of this dictionary are those
    that were returned by the core.multi_class_split function and a few new 
    ones:
    "AXIS" : the best feature to perform the split on
    "TYPE" : the type of split, "UNIVARIATE" in this case
    "SPLIT_AT_VALUE" :  the value of the variable indicated by
    result_dict["AXIS"] that is used to split the data (and is later used by 
    the classifier)
    """
    assert MIN_LEAF_SIZE > 0
    assert type(MIN_LEAF_SIZE) == type(1)
    assert type(total_weight) == type([])
    n_dim = len(data[0]) - 3 
    result_dict = {"GINI" : 3 * sum(total_weight)}

    for i in xrange(n_dim):
        # The following line sorts first fot the relevant variable and then
        # if there are several values that are the samne it sorts for the 
        # unique id that each datapoint (or example) has, this ensures that
        # sorting yields the same result always.
        data.sort(key = lambda x: (x[i], x[-3]))
        r = multi_class_split(data, total_weight, MIN_LEAF_SIZE)
        if r["GINI"] < result_dict["GINI"]:
            result_dict = r
            result_dict["AXIS"] = i
            result_dict["TYPE"] = "UNIVARIATE"
            slice = result_dict["SPLIT"]
            result_dict["SPLIT_AT_VALUE"] =  0.5 * (data[slice - 1][i] + data[slice][i]) 

    assert "SPLIT_AT_VALUE" in result_dict
    return result_dict



def forest_RI(data, total_weight, MIN_LEAF_SIZE, F):
    """This function performs a split in the way that the forest_RI method
    of constructing classifiers needs. It randomly chooses F variables from 
    the measurement vectors and tests splitting on each of those F 
    variables, keeping the one that performs the best split.
    
    input:
    data -- a list of tuples or lists with n + 3 entries for data that has 
    n variables. The other three entries, the last and the second to last 
    and the third to last are respectively reserved for the class label and the 
    weight and an unique id (needed during sorting, of which this library does a
    lot). The class labels are integers starting at 0 and going to n_class - 1. 
    total_weight -- list of floats (or integers), the total weight of all the 
    datapoints in the data (one entry for each class, indexed by class label).
    Passing this to the function avoids having a loop to recalculate it.
    MIN_LEAF_SIZE -- integer, minimum leaf node size for the decision tree 
    that is constructed by this function. Trees with too small leaf node sizes 
    generalize badly to new data.
    F -- Integer, the number of features to take into account. This function
    will try F features at random and keep the split found for the best among 
    them.
    
    output: 
    result_dict -- a dictionary containing all the relevant data about the 
    split generated by this function. The keys of this dictionary are those
    that were returned by the core.multi_class_split function and a few new 
    ones:
    "AXIS" : the best feature to perform the split on
    "TYPE" : the type of split, "UNIVARIATE" in this case
    "SPLIT_AT_VALUE" :  the value of the variable indicated by
    result_dict["AXIS"] that is used to split the data (and is later used by 
    the classifier)    
    """
    
    
    tmp_mask = range(len(data[0]) - 3)
    random.shuffle(tmp_mask)
    mask = tmp_mask[0:F]    
    result_dict = {"GINI" : 3 * sum(total_weight)}

    for i in mask:
        data.sort(key = lambda x: (x[i], x[-3]))
        r = multi_class_split(data, total_weight, MIN_LEAF_SIZE)
        if r["GINI"] < result_dict["GINI"]:
            result_dict = r
            result_dict["AXIS"] = i
            result_dict["TYPE"] = "UNIVARIATE"
            slice = result_dict["SPLIT"]
            result_dict["SPLIT_AT_VALUE"] =  0.5 * (data[slice - 1][i] + data[slice][i]) 

    assert "SPLIT_AT_VALUE" in result_dict
    return result_dict

if __name__ == "__main__":
    print __doc__